{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Case Study: American Community Survey Data\n",
        "\n",
        "Let's look at an example of how we might utilize these workflows on an actual dataset, specifically <a href=\"https://data.census.gov/table/ACSST5Y2022.S1501?t=Education:Educational Attainment&g=050XX00US18141,18141$8600000&moe=false\">educational outcome attribute data from the American Community Survey</a>\n",
        "  * [Link to download](https://raw.githubusercontent.com/kwaldenphd/elements-of-computing/main/book/data/ch5/data.csv)\n",
        "  * *Note: I'm working with 2022's 5 year estimate, which I've renamed `data.csv`.*"
      ],
      "metadata": {
        "id": "vDEa9oue3mTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd, re # import statements\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/kwaldenphd/elements-of-computing/main/book/data/ch5/data.csv\") # load data\n",
        "df # show output"
      ],
      "metadata": {
        "id": "0gHuIlMi45V5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A rich dataset, but not in particularly usable shape at the moment.\n"
      ],
      "metadata": {
        "id": "vGVZuIii5BFm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step #1: Building an Outline\n",
        "\n",
        "A good starting point is sketching out how you want the data to look at the end of reshaping operations. We can start thinking of this as the data processing version of pseudocode.\n",
        "\n",
        "For this dataset, you might want something like....\n",
        "\n",
        "| Area | Category | Variable | Gender | Value |\n",
        "| --- | --- | --- | --- | --- |\n",
        "| *geographic unit* | *category* | *specific field* | *gender breakdown* | *amount or measure* |\n",
        "\n",
        "Based on where the data is now, some things we might need to do...\n",
        "- Take the category rows, make them a new column, and create a multi-level index on that axis\n",
        "- Break out the current column headers\n",
        "- Make the updated column headers their own columns\n",
        "- Probably relabel the columns to more user-friendly labels\n",
        "\n",
        "Let's dig in!"
      ],
      "metadata": {
        "id": "_ip7lBDQ6iA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step #2: Breaking out category and variable columns\n",
        "\n",
        "Let's take a look at the first column."
      ],
      "metadata": {
        "id": "MOAf6tHtVcfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[:, 0].unique() # inspect unique values in first column"
      ],
      "metadata": {
        "id": "BjFefFVZVt_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A few things going on here, but the first thing we might want to do is remove the `\\xa0` unicode character."
      ],
      "metadata": {
        "id": "cJKF8xigWACW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.replace(r\"\\xa0\", '', regex=True, inplace=True) # remove unicode character\n",
        "df.iloc[:, 0].unique() # inspect output"
      ],
      "metadata": {
        "id": "L35WuX_pWTCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we can use a couple conditional statements to add a `Category` column.\n",
        "- First, we'll insert an empty column at the start of our DataFrame\n",
        "- Then, we'll iterate over rows, test if the third cell is null (`NaN`)\n",
        "- If that test is `True`, we'll add the category value to the first column"
      ],
      "metadata": {
        "id": "rjfm8M4zWcSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.insert(0, 'Category', '', True) # insert new column for category\n",
        "\n",
        "for i, r in df.iterrows(): # iterate over rows, insert new column value\n",
        "  if pd.isnull(r.iloc[2]) == True: # test if third cell is a NaN value\n",
        "    df['Category'].iloc[i] = r.iloc[1] # if null test if True, add that value to a new column\n",
        "  else: # otherwise, keep iterating over rows\n",
        "    continue\n",
        "\n",
        "df # inspect output"
      ],
      "metadata": {
        "id": "Ahvcrvqpbsy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making progress! Let's break down the pseudocode version of next steps:\n",
        "- Now that we have a `Category` column, we can filter out the `NaN` rows.\n",
        "- Then, we can forward fill or fill down the `Category` column values so there's a connection between the category and variable lable\n",
        "\n",
        "A couple last steps before we think about the next reshaping operation:\n",
        "- Rename the second column\n",
        "- Reset the index (after filtering)"
      ],
      "metadata": {
        "id": "UFY7RyYfb1BX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.mask(df == '') # mask empty cells with NaN\n",
        "df.iloc[:, 0].ffill(inplace=True) # foward fill category column\n",
        "df.dropna(inplace=True) # drop rows with NA\n",
        "df.columns.values[1] = 'Variable' # rename column\n",
        "df.reset_index(inplace=True, drop=True) # reset index\n",
        "df # show output"
      ],
      "metadata": {
        "id": "2izQLKLyct35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step #3: Melting Variable Labels\n",
        "\n",
        "Now that we have a `Category` and `Variable` columns, let's melt this dataframe so the existing column labels become their own column. We'll use these two columns as `id_vars`.\n",
        "\n",
        "We can also rename the columns after this operation."
      ],
      "metadata": {
        "id": "gpJntaitd2-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.melt(df, id_vars=['Category', 'Variable']) # melt everything but the first two columns\n",
        "df.columns = ['category', 'variable', 'metadata', 'value']\n",
        "df # inspect output"
      ],
      "metadata": {
        "id": "J_-NE8Asdt1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step #4: Splitting multi-value column\n",
        "\n",
        "If we take a closer look at the `metadata` column, there are three key pieces of information:\n",
        "- Geographic scope (county, zip code, etc)\n",
        "- Gender scope (total, male, female)\n",
        "- Measure type (all estimates, so we'll eventually drop this value)\n",
        "\n",
        "We'll use `!!` as a separator for regular expression string methods.\n",
        "- [Click here](https://colab.research.google.com/drive/1Rd9KMoJ2AdtwVMNXwibakUbMGxz5Z6nM?usp=sharing#scrollTo=c1TKLsmiSS9N) for a Jupyter Notebook that provides a deep dive into regular expressions (regex) and string methods in Python."
      ],
      "metadata": {
        "id": "ZqupsOvHeiLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[['area', 'gender', 'measure']] = df['metadata'].str.split('!!', expand=True) # split metadata column using separator\n",
        "df # inspect output"
      ],
      "metadata": {
        "id": "CwkrId3Ke9wZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step #5: Subsetting & Filtering\n",
        "\n",
        "Last but not least, we might want to subset our `DataFrame` for meaningful columns, and remove rows with `NaN` values."
      ],
      "metadata": {
        "id": "M2O-7z5c9xcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[['area', 'category', 'variable', 'gender', 'value']] # subset columns\n",
        "df = df[df['value'].notnull()] # remove rows with NaN in value\n",
        "df = df.reset_index(drop=True) # reset index\n",
        "df['area'] = df['area'].str.replace(\"ZCTA5 \", \"\") # clean up area column to be able to join on zip code\n",
        "df # show output"
      ],
      "metadata": {
        "id": "HEVBqOoV-MJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's more we might want to do with processing or filtering this data, but a solid start to programmatically reshape this data."
      ],
      "metadata": {
        "id": "ibk9otPyfs3A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wrap Up\n",
        "\n",
        "Voila! Let's see all of those steps together:"
      ],
      "metadata": {
        "id": "74scNy3v-499"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd, re # import statements\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/kwaldenphd/elements-of-computing/main/book/data/ch5/data.csv\") # load data\n",
        "\n",
        "df.replace(r\"\\xa0\", '', regex=True, inplace=True) # remove unicode character\n",
        "\n",
        "df.insert(0, 'Category', '', True) # insert new column for category\n",
        "\n",
        "for i, r in df.iterrows(): # iterate over rows, insert new column value\n",
        "  if pd.isnull(r.iloc[2]) == True: # test if third cell is a NaN value\n",
        "    df['Category'].iloc[i] = r.iloc[1] # if null test if True, add that value to a new column\n",
        "  else: # otherwise, keep iterating over rows\n",
        "    continue\n",
        "\n",
        "df = df.mask(df == '') # mask empty cells with NaN\n",
        "df.iloc[:, 0].ffill(inplace=True) # foward fill category column\n",
        "df.dropna(inplace=True) # drop rows with NA\n",
        "df.columns.values[1] = 'Variable' # rename column\n",
        "df.reset_index(inplace=True, drop=True) # reset index\n",
        "\n",
        "df = pd.melt(df, id_vars=['Category', 'Variable']) # melt everything but the first two columns\n",
        "df.columns = ['category', 'variable', 'metadata', 'value']\n",
        "df[['area', 'gender', 'measure']] = df['metadata'].str.split('!!', expand=True) # split metadata column using separator\n",
        "\n",
        "df = df[['area', 'category', 'variable', 'gender', 'value']] # subset columns\n",
        "df = df[df['value'].notnull()] # remove rows with NaN in value\n",
        "df = df.reset_index(drop=True) # reset index\n",
        "df['area'] = df['area'].str.replace(\"ZCTA5 \", \"\") # clean up area column to be able to join on zip code\n",
        "df.to_csv(\"output.csv\", index=False) # write output to csv\n",
        "df # show output"
      ],
      "metadata": {
        "id": "h94enSTA-_Fs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}