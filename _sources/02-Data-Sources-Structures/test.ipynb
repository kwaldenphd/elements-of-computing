{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1226f888",
   "metadata": {},
   "source": [
    "# Elements of Computing (S24): Data Sources (part 1)\n",
    "\n",
    "<a href=\"http://creativecommons.org/licenses/by-nc/4.0/\" rel=\"license\"><img style=\"border-width: 0;\" src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" alt=\"Creative Commons License\" /></a>\n",
    "This tutorial is written by [Katherine Walden](https://github.com/kwaldenphd) and is licensed under a <a href=\"http://creativecommons.org/licenses/by-nc/4.0/\" rel=\"license\">Creative Commons Attribution-NonCommercial 4.0 International License</a>.\n",
    "\n",
    "## Acknowledgements\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "## How to Use This Notebook\n",
    "\n",
    "First, make sure you're working in the copy of the notebook that has your name (check the file name in the top left-hand corner of the screen).\n",
    "\n",
    "We're using a Python authoring environment called **a notebook**, which is made up of blocks or cells.\n",
    "- Text cells (like this one), consist of formatted text. For the most part, these cells include instructions and you will just read them.\n",
    "- Code cells (like the sample one below) have Python code that you will run. To run a code cell, hover over it with the cursor and click the `arrow` icon.\n",
    "\n",
    "SCREENSHOT\n",
    "\n",
    "If at any point you want to add a new code cell to try things out on your own, there are a couple ways to add a code cell. First, you'll need to click on a text or code cell near where you want to add a cell:\n",
    "- `+ Code` in the upper left-hand corner of the screen, under `Help`\n",
    "- `Insert -> Code cell` in the upper left-hand menu bar\n",
    "\n",
    "## Lab Notebook Template\n",
    "\n",
    "Moving forward, we're going to be submitting lab notebooks using the provide Jupyter Notebook template.\n",
    "- If working in JupyterLab (or another desktop IDE), download the `.ipynb` file to your local computer\n",
    "  * `File` - `Download` - `Download as .ipynb`\n",
    "- If working in Google Colaboratory, MAKE SURE you save a copy to your local drive. Otherwise your changes will not be saved.\n",
    "  * `File` - `Save a copy in Drive`\n",
    "\n",
    "The lab notebook template includes all of the questions as well as pre-created markdown cells for narrative text answers and pre-created code cells for any programs you may need to create.\n",
    "- Double click on these cells to edit and add your own content\n",
    "- If questions do not require a code component, you can ignore those cells\n",
    "- If questions to not require a narrative component, you can ignore those cells\n",
    "\n",
    "If working in JupyterLab or another desktop IDE, upload the lab notebook template `.ipynb` file to Canvas as your lab submission.\n",
    "\n",
    "If working in Google Colaboratory, submit the link to your notebook (checking sharing permissions, similar with Google Docs).\n",
    "\n",
    "## Overview\n",
    "\n",
    "<p align=\"center\"><img src=\"https://github.com/kwaldenphd/python-data-structures/blob/main/images/Python_Data_Structures.png?raw=true\" width=\"1000\"></p>\n",
    "\n",
    "In this lab, we're thinking about how Python's built-in data structures (along with `Pandas` workflows) connect to how data is often structured in files or made available on the web.\n",
    "\n",
    "We'll also be covering how to access data via web APIs, and preliminary workflows for text mining & documents.\n",
    "\n",
    "## Structured Data\n",
    "\n",
    "Specifically, we're thinking about a couple key data structures:\n",
    "- Tabular data, or data stored as a table consisting of rows and columns\n",
    "- Data stored as name-value pairs, specifically in the JavaScript Object Notation (`.json`) format\n",
    "\n",
    "<table>\n",
    "<tr><td><p align=\"center\"><img src=\"https://github.com/kwaldenphd/python-structured-data/blob/main/images/One_Dimensional_Array.jpg?raw=true\" width=\"500\"></p></td><td><p align=\"center\"><img src=\"https://github.com/kwaldenphd/python-structured-data/blob/main/images/Associative_Arrays.png?raw=true\" width=\"500\"></p></td></tr>\n",
    "\t<tr><td align=\"center\">Linear Array</td><td align=\"center\">Associative Array</td></tr></table>\n",
    "\n",
    "We can connect these two types of data structures back to the concepts of linear and associative arrays covered previously in the course. A quick refresher:\n",
    "- \"In computer science, an array is a data structure consisting of a collection of elements (values or variables), each identified by at least one array index or key...The simplest type of data structure is a linear array, also called one-dimensional array\" ([Wikipedia, \"Array (data structure)](https://en.wikipedia.org/wiki/Array_(data_structure))\")\n",
    "- Associative arrays are \"an abstract data type that stores a collection of (key, value) pairs, such that each possible key appears at most once in the collection\" ([Wikpedia, Associative Array](https://en.wikipedia.org/wiki/Associative_array))\n",
    "\n",
    "### Tabular (or Delimited) Data\n",
    "\n",
    "<p align=\"center\"><img src=\"https://github.com/kwaldenphd/python-structured-data/blob/main/images/table_diagram.png?raw=true\" width=\"1000\"></p>\n",
    "\n",
    "\"A delimited file is a sequential file with column delimiters. Each delimited file is a stream of records, which consists of fields that are ordered by column. Each record contains fields for one row. Within each row, individual fields are separated by column delimiters. All fields must be delimited character strings, non-delimited character strings, or external numeric values. Delimited character strings can contain column delimiters and can also contain character string delimiters when two successive character string delimiters are used to represent one character.\" (IBM, \"[Delimited File Format](https://www.ibm.com/docs/en/db2-for-zos/11?topic=utilities-delimited-file-format)\", 5 February 2022)\n",
    "\n",
    "A few characteristics that distinguish `.csv` files (or other plain-text delimited data formats) from proprietary spreadsheet file types:\n",
    "- Columns in a `.csv` file don't have a value type. Everything is a string.\n",
    "- Values in a `.csv` file don't have font or color formatting\n",
    "- `.csv` files only contain single worksheets\n",
    "- `.csv` files don't store formatting information like cell width/height\n",
    "- `.csv` files don't recognize merged cells or other kinds of special formatting (frozen or hidden rows/columns, embedded images, etc.)\n",
    "\n",
    "### JavaScript Object Notation\n",
    "\n",
    "START HERE: https://youtu.be/B8AvytgCBdE?si=SIkXF1u7xFOznfv0\n",
    "\n",
    "<p align=\"center\"><img src=\"https://github.com/kwaldenphd/python-structured-data/blob/main/images/JSONSample.jpg?raw=true\" width=\"500\"></p>\n",
    "\n",
    "JavaScript Object Notation (JSON) is as popular way to format data as a single (purportedly human-readable) string. JavaScript programs use JSON data structures, but we can frequently encounter JSON data outside of a JavaScript environment.\n",
    "\n",
    "Websites that make machine-readable data available via an application programming interface (API- more on this soon) often provide that data in a JSON format. JSON structures can vary WIDELY depending on the specific data provider, but the easiest way to think of JSON data as a plain-text data format made up of something like key-value pairs, like we've encountered previously in working with dictionaries (as a type of associative array).\n",
    "\n",
    "Example JSON string: `stringOfJsonData = '{\"name\": Zophie\", \"isCat\": true, \"miceCaught\": 0, \"felineIQ\": null}'`. From looking at the example string, we can see field names or keys (`name`, `isCat`, `miceCaught`, `felineIQ`) and values for those fields.\n",
    "\n",
    "To use more precise terminology, JSON data has the following attributes:\n",
    "- uses name/value pairs\n",
    "- separates data using commas\n",
    "- holds objects using curly braces `{}`\n",
    "- holds arrays using square brackets `[]`\n",
    "\n",
    "In our example `stringOfJsonData`, we have an object contained in curly braces. An object can include multiple name/value pairs. Multiple objects together can form an array.\n",
    "\n",
    "How is data stored in a JSON format different than a `.csv`?\n",
    "- A `.csv` file uses characters as delimiters and has more of a tabular (table-like) structure.\n",
    "- `.json` data uses characters as part of the syntax, but not in the same way as delimited data files.\n",
    "- The data stored in a JSON format has values that are attached to names (or keys).\n",
    "  * NOTE: We can mimic this structure somewhat by loading a `.csv` as a dictionary data structure\n",
    "- JSON can also have a hierarchical or nested structure, in that objects can be stored or nested inside other objects as part of the same array.\n",
    "\n",
    "### Loading Structured Data in Python\n",
    "\n",
    "Much of our work this semester will utilize `Pandas`, a powerful Python library for working with structured data. But we can also work with structured data using Python's built-in secondary data structures and the `csv` and `json` modules.\n",
    "\n",
    "#### Without Pandas\n",
    "\n",
    "##### `csv`\n",
    "\n",
    "The `csv` module allows us to create a `reader` object that iterates over lines in a `.csv` file. We can use it to read or load an existing `.csv` file into Python. \n",
    "\n",
    "```Python\n",
    "# loading delimited data as list of lists\n",
    "import csv # import statement\n",
    "file = open('example.csv') # open file\n",
    "reader = csv.reader(file) # create reader object\n",
    "data = list(reader) # create list of lists\n",
    "print(data) # show output\n",
    "```\n",
    "\n",
    "```Python\n",
    "# if we wanted to combine those steps\n",
    "import csv # import statement\n",
    "data = list(csv.reader(open('example.csv'))) # open file, create reader object, convert to list\n",
    "print(data) # show output\n",
    "```\n",
    "\n",
    "For more on using the `csv` module and this workflow:\n",
    "- [Elements of Computing I lab notes](https://github.com/kwaldenphd/python-structured-data/tree/main#read)\n",
    "\n",
    "```Python\n",
    "# loading delimited data as dictionary\n",
    "import csv # import statement\n",
    "file = open('example.csv') # open file\n",
    "dictReader = csv.DictReader(file) # DictReader object\n",
    "data = list(dictReader) # convert DictReader to list of dictionaries\n",
    "print(data) # show output\n",
    "```\n",
    "\n",
    "```Python\n",
    "# if we wanted to combine those steps\n",
    "import csv # import statement\n",
    "data = list(csv.DictReader(open('example.csv'))) # open file, create DictReader object, convert to list of dictionaries\n",
    "print(data) # show output\n",
    "```\n",
    "\n",
    "##### `json`\n",
    "\n",
    "We can read JSON into Python using the `json` module, which includes a few key functions for loading JSON data into Python:\n",
    "- `json.loads()` takes a single string of JSON and loads it as a Python value\n",
    "- `json.load()` takes a JSON file (or file-like object) and loads it as a Python value\n",
    "- `json.dumps()` takes a Python value and transforms it to a JSON object.\n",
    "\n",
    "Values stored in JSON format must be one of the following data types:\n",
    "- string\n",
    "- number\n",
    "- object (JSON object)\n",
    "- array\n",
    "- boolean\n",
    "- null\n",
    "\n",
    "Translation table for how Python's `json` module interprets or renders these data types:\n",
    "\n",
    "JSON | Python\n",
    "--- | ---\n",
    "object | dict\n",
    "array | list\n",
    "string | str\n",
    "number (int) | int\n",
    "number (real) | float\n",
    "true | True\n",
    "false | False\n",
    "null | None\n",
    "\n",
    "To translate a string of JSON data into a Python value, we pass it to the `loads()` function contained in the `json` module.\n",
    "\n",
    "```Python\n",
    "import json # import statement\n",
    "jString = '{\"name\": \"Zophie\", \"isCat\": true, \"miceCaught\": 0, \"felineIQ\": null}' # string of JSON data\n",
    "data = json.loads(jString) # load JSON data as Python value\n",
    "print(data) # show output\n",
    "```\n",
    "\n",
    "When loading a JSON file (or file-like object), we would need to use the `json.load()` argument.\n",
    "\n",
    "```Python\n",
    "import json # import statement\n",
    "file = open(\"example.json\") # open file\n",
    "data = json.load(file) # parse as JSON\n",
    "print(data) # show output\n",
    "```\n",
    "\n",
    "For more on using the `json` module and this workflow:\n",
    "- [Elements of Computing I lab notes](https://github.com/kwaldenphd/python-structured-data/tree/main#javascript-object-notation)\n",
    "\n",
    "If you're feeling fuzzy on file methods:\n",
    "- [Elements of Computing I lab notes](https://github.com/kwaldenphd/python-structured-data/tree/main#overview)\n",
    "\n",
    "#### With Pandas\n",
    "\n",
    "<p align=\"center\"><img src=\"https://github.com/kwaldenphd/pandas-intro/blob/main/images/series_df_diagram.png?raw=true\" width=\"1000\"></p>\n",
    "\n",
    "`pandas` has two main data structures: `Series` and `DataFrame`.\n",
    "- \"A `Series` is a one-dimensional, array-like object containing a sequence of values...and an associated array of data labels, called its index\" (McKinney, 126)\n",
    "- A `DataFrame` includes a tabular data structure \"and contains an ordered collection of columns, each of which can be a different value type\" (McKinney, 130).\n",
    "\n",
    "##### Delimited file -> `DataFrame`\n",
    "\n",
    "Loading a delimited file as a DataFrame is fairly straightforward.\n",
    "\n",
    "```Python\n",
    "import pandas as pd # import statement\n",
    "df = pd.read_csv(\"example.csv\") # load file, create dataframe\n",
    "print(df) # show output\n",
    "```\n",
    "\n",
    "`.read_csv()` can include a number of other arguments or parameters to handle different data loading challenges, for example other delimiter characters, escape characters, and data type conversion. For more on parsing delimited data in `Pandas`:\n",
    "- [Pandas documentation](https://pandas.pydata.org/docs/user_guide/io.html)\n",
    "\n",
    "The `read_` prefix can be used with other structured data file formats. For more on `Pandas` and file I/O:\n",
    "- [Pandas documentation](https://pandas.pydata.org/docs/user_guide/io.html)\n",
    "\n",
    "##### `Pandas` & `JSON`\n",
    "\n",
    "Since a `DataFrame` is a flat two-dimensional structure, highly-nested JSON can present a challenge.\n",
    "\n",
    "Pandas includes a few JSON-specific functions. Documentation links are included below.\n",
    "- [Reading JSON](https://pandas.pydata.org/docs/user_guide/io.html#reading-json)\n",
    "- [.read_json()](https://pandas.pydata.org/docs/reference/api/pandas.read_json.html)\n",
    "- [.json_normalize()](https://pandas.pydata.org/docs/reference/api/pandas.json_normalize.html#pandas.json_normalize)\n",
    "\n",
    "We'll come back to these functions when we start working with web API returns.\n",
    "\n",
    "### Application\n",
    "\n",
    "Q1A: Navigate to an open data portal and download a `.csv` file. \n",
    "\n",
    "A few places to start:\n",
    "- [Data.gov](https://www.data.gov/)\n",
    "- [City of Chicago Data Portal](https://data.cityofchicago.org/)\n",
    "- [City of South Bend Open Data](https://data-southbend.opendata.arcgis.com/)\n",
    "\n",
    "These open data portals are catalogs of datasets- you will need to explore the websites to identify and then download a specific dataset.\n",
    "\n",
    "Open the data in a spreadsheet program and/or text editor. \n",
    "- What do you see?\n",
    "- How can we start to make sense of the data based on available documentation?\n",
    "\n",
    "Q1B: Write three programs that load the data in Python using the different approaches highlighted in the previous section of the lab:\n",
    "- Lists and sublists\n",
    "- Dictionaries\n",
    "- Pandas `DataFrame`\n",
    "\n",
    "Answer to this question includes program + comments that document process and explain your code.\n",
    "\n",
    "Q1C: What challenges did you encounter? How did you address or solve them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e756f28",
   "metadata": {},
   "source": [
    "Q2A: Navigate to an open data portal and download a JSON file. \n",
    "\n",
    "Some options that can get you started:\n",
    "- [Data.gov](https://www.data.gov/)\n",
    "- [City of Chicago Data Portal](https://data.cityofchicago.org/)\n",
    "- [City of South Bend Open Data](https://data-southbend.opendata.arcgis.com/)\n",
    "\n",
    "These open data portals are catalogs of datasets- you will need to explore the websites to identify and then download a specific dataset. \n",
    "\n",
    "Q2B: Open the data in a spreadsheet program and/or text editor. Describe what are you seeing. How can we start to make sense of this data? What documentation is available?\n",
    "\n",
    "Q2C: Write a program that loads the JSON data into Python, using the workflow shown in the previous section of the lab.\n",
    "- You're welcome to explore creating a Pandas `DataFrame`, but that's not required.\n",
    "\n",
    "Answer to this question includes program + comments that document process and explain your code.\n",
    "\n",
    "Q3C: What challenges did you encounter? How did you address or solve them? \n",
    "\n",
    "## Web APIs\n",
    "\n",
    "START HERE (YouTube): https://youtu.be/qW1qhb8r8xI?si=q8jdyE6oZEsH4xFR\n",
    "\n",
    "For a brief introduction to APIs, view Danielle Th√©, [\"API's Explained (with LEGO)\"](https://youtu.be/qW1qhb8r8xI), *YouTube Video* (1 November 2016).\n",
    "\n",
    "<p align=\"center\"><a href=\"https://github.com/kwaldenphd/apis-python/blob/main/Figure_1.jpg?raw=true\"><img class=\"aligncenter\" src=\"https://github.com/kwaldenphd/apis-python/blob/main/Figure_1.jpg?raw=true\" /></a></p>\n",
    "\n",
    "We're focusing on web APIs, which allow for information or functionality to be manipulated by other programs via the internet. Web APIs are a popular workflow for making data available from large-scale repositories. A few examples...\n",
    "\n",
    "- The U.S. National Archives and Records Administration (NARA) makes the National Archives catalog, Executive Orders, and photographic image content available [via an API](https://www.archives.gov/developer#toc--datasets).\n",
    "- The Smithsonian Institution allows open access to museum collection metadata [via an API](https://www.si.edu/openaccess/devtools).\n",
    "- The U.S. Library of Congress provides access to digital collection metadata, digitized historical newspaper images, public radio and television programs, and World Digital Library collections [via APIs](https://labs.loc.gov/lc-for-robots/). \n",
    "- The U.S. Census Bureau makes a wide range of public data available [via API](https://www.census.gov/data/developers/data-sets.html).\n",
    "\n",
    "APIs let us bring a live data connection into a programming environment and interact or work with it based on the format and protocols established as part of the API.\n",
    "\n",
    "For example, imagine you want to know winter weather road conditions or the status of snow plow routes. Downloading a static dataset isn't going to work for this specific use case--you need a live data feed with up-to-date information. This is the beauty of APIs!\n",
    "- [Track down Iowa Department of Transportation's nearly 901 snow plows](https://public-iowadot.opendata.arcgis.com/pages/winter). \n",
    "- [Link to IDOT's plow truck data](https://public-iowadot.opendata.arcgis.com/datasets/IowaDOT::snow-plow-truck-location-avl-iowa-dot/about)\n",
    "\n",
    "### API Terminology & General Workflow\n",
    "\n",
    "Some terminology that goes along with APIs.\n",
    "- ***HTTP (HyperText Transfer Protocol)***: primary means of communicating or transferring data on the world wide web. \n",
    "- ***URL (Uniform Resource Locator)***: an address or location information for information on the web. A URL describes the location of a specific resource.\n",
    "- ***JSON (JavaScript Object Notation)***: plain-text data storage format\n",
    "- ***REST (REpresentational State Transfer)***: best practices for implementing APIs. APIs that follow these principles are called REST APIs.\n",
    "\n",
    "A very general web API workflow....\n",
    "- Request data from a URL\n",
    "  * Sometimes you have to specify additional parameters, like headers or an access key/token.\n",
    "- Store the response in Python\n",
    "- Convert the response to the appropriate Python data structure\n",
    "\n",
    "Let's look at a couple of examples. \n",
    "\n",
    "### City of South Bend Open Data Portal\n",
    "\n",
    "OPEN DATA PORTAL SCREENSHOT\n",
    "\n",
    "Head to the City of South Bend's Open Data Portal. Identify a specific dataset you want to load in Python.\n",
    "\n",
    "DATA INVENTORY SCREENSHOT\n",
    "\n",
    "I'm working with the raw data from the City's 2022 Digital Literacy Survey.\n",
    "- https://data-southbend.opendata.arcgis.com/datasets/SouthBend::2022-digital-literacy-survey-raw-data/about\n",
    "\n",
    "VIEW API RESOURCES SCREENSHOT\n",
    "\n",
    "We could download this data as a `CSV` file, but let's instead explore the API options.\n",
    "\n",
    "API EXPLORER SCREENSHOT\n",
    "\n",
    "https://data-southbend.opendata.arcgis.com/datasets/SouthBend::2022-digital-literacy-survey-raw-data/api\n",
    "\n",
    "QUERY URL SCREENSHOT \n",
    "\n",
    "Spend some time exploring the options on this page. As you customize the query, notice how the `Query URL` changes. We can think about the `URL` as the data's \"address.\" Modifying what data we want requires modifying the address.\n",
    "\n",
    "Once you have finalized your query (what parts of this data you want to work with in Python), we'll use the `requests` and `json` modules to load the data.\n",
    "\n",
    "```Python\n",
    "import requests, json # import statements\n",
    "r = requests.get(\"https://opendata.arcgis.com/datasets/c97085b608604f5c8c07487c24dcaff4_0/FeatureServer/0/query?where=1%3D1&outFields=*&outSR=4326&f=json\") # request data from url\n",
    "data = r.json() # parse response as JSON object\n",
    "print(data) # show output\n",
    "```\n",
    "\n",
    "We can use Python dictionary syntax to make sense of what's in this output.\n",
    "\n",
    "```Python\n",
    "# show top-level keys\n",
    "print(data.keys())\n",
    "```\n",
    "\n",
    "At first glance, everything up through the `fields` key looks like technical information about the API return. `fields` doesn't have the survey responses, but it does have important metadata about the survey data.\n",
    "\n",
    "Let's start by creating a `DataFrame` with this `metadata` (data about data).\n",
    "\n",
    "```Python\n",
    "metadata = pd.DataFrame(data['fields']) # create dataframe\n",
    "metadata.to_csv(\"metadata.csv\", index=False) # write metadata to csv file\n",
    "print(metadata) # show output\n",
    "```\n",
    "\n",
    "The `fields` key includes survey response data, but it's structured as a list of dictionaries. We'll need to use a combination of list and dictionary Python syntax (and some trial and error) here.\n",
    "\n",
    "```Python\n",
    "print(type(data['features'])) # shows us data type for the value associated with the features key\n",
    "```\n",
    "\n",
    "```Python\n",
    "response = data['features'][0] # selects the first response\n",
    "print(type(response)) # show response data type\n",
    "```\n",
    "```Python\n",
    "print(response.keys()) # shows keys for single response\n",
    "```\n",
    "```Python\n",
    "print(type(response['attributes'])) # show data type for attributes value\n",
    "```\n",
    "```Python\n",
    "print(response['attributes'].keys()) # gets value for attributes key, shows those keys\n",
    "```\n",
    "\n",
    "That's starting to look like survey results. We could use another print statement to confirm.\n",
    "\n",
    "```Python\n",
    "for key, value in response['attributes'].items(): # iterate over dictionary items\n",
    "  print(key, value) # show key and value\n",
    "```\n",
    "\n",
    "Now that we understand how the API return data is structured, we can create a `DataFrame` with the survey data. We'll first need a list with the survey response dictionaries.\n",
    "\n",
    "```Python\n",
    "import pandas as pd # import statement\n",
    "\n",
    "surveyData = [] # empty list for responses\n",
    "\n",
    "for d in data['features']: # iterate over list of dictionaries\n",
    "  surveyData.append(d['attributes']) # isolate value and append to list\n",
    "\n",
    "df = pd.DataFrame(surveyData) # create df\n",
    "print(df) # show output\n",
    "```\n",
    "\n",
    "Now, there's more we might want to do with this `DataFrame` (rename columns, remove unneeded columns, perform other kinds of calculations, etc). But this gets us started!\n",
    "\n",
    "To recap what we just did:\n",
    "- Customize query URL\n",
    "- Load response in Python\n",
    "- Parse the data structure to isolate the data we want\n",
    "- Store that data as a Pandas `DataFrame`\n",
    "\n",
    "#### Application\n",
    "\n",
    "Q3: \n",
    "\n",
    "Work through this for another open data portal dataset. You're welcome to use another City of South Bend dataset, and it's fine to continue with the Q1 dataset. \n",
    "\n",
    "But if you want to flex you API muscles, try a different open data portal to learn a new interface and data structure! Lots to choose from, but Chicago and NYC are good places to start.\n",
    "- [Chicago Data Portal](https://data.cityofchicago.org/)\n",
    "  * [Documentation](https://dev.socrata.com/docs/endpoints.html)\n",
    "- [NYC Open Data](https://opendata.cityofnewyork.us/)\n",
    "  * [Documentation](https://dev.socrata.com/docs/endpoints.html) \n",
    "\n",
    "Starting to make sense of the documentation\n",
    "\n",
    "Getting the data in Python\n",
    "\n",
    "Storing as pandas df\n",
    "\n",
    "### U.S. Census Bureau\n",
    "\n",
    "As we've seen previously, the U.S. Census Bureau collects a wealth of information about people, households, communities, etc. They also have a [number of APIs](https://www.census.gov/data/developers/data-sets.html) and a [robust user community](https://www.census.gov/data/what-is-data-census-gov/guidance-for-data-users/how-to-materials-for-using-the-census-api.html).\n",
    "\n",
    "<p align=\"center\"><a href=\"https://github.com/kwaldenphd/apis-python/blob/main/Figure_2.png?raw=true\"><img class=\"aligncenter\" src=\"https://github.com/kwaldenphd/apis-python/blob/main/Figure_2.png?raw=true\" /></a></p>\n",
    "\n",
    "Unlike the APIs we encountered in the previous section of the lab, the Census Bureau's API requires a key. Head to the Census Bureau's [\"About\"](https://www.census.gov/data/developers/about.html) page for developers and click the \"Request a Key\" icon. Once you've completed the \"Request a Key\" form, you should receive an API key via email. This will look like a long string of letters and numbers.\n",
    "- *It's fine to use your name or \"University of Notre Dame\" for \"Organization Name\"*\n",
    "\n",
    "#### Selecting a Dataset\n",
    "\n",
    "TABLE INVENTORY SCREENSHOT\n",
    "\n",
    "The first step is figuring out what Census Bureau dataset and API you want to work with. One option is to use [data.census.gov](https://data.census.gov/)'s data explorer interface. Keyword search results can point you to datasets of interest (which you would then have to track down in the API documentation).\n",
    "\n",
    "AVAILABLE APIS SCREENSHOT\n",
    "\n",
    "https://www.census.gov/data/developers/data-sets.html\n",
    "\n",
    "Looking at the APIs for [specific Census Bureau data collection instruments](https://www.census.gov/data/developers/data-sets.html) is also a place to start. \n",
    "\n",
    "DOISCOVERY TOOL SCREENSHOT \n",
    "\n",
    "You could also jump into the [API Discovery Tool](https://www.census.gov/data/developers/updates/new-discovery-tool.html), which is available in a couple different formats. This is a machine-readable inventory of Census Bureau datatasets. It can also be slightly overwhelming.\n",
    "\n",
    "For this example, I'm going to use microdata from the American Community Survey, specifically the \"[2022 American Community Survey 1-Year Estimates Public Use Microdata Sample (2005-2019, 2021, 2022)](https://www.census.gov/data/developers/data-sets/census-microdata-api.ACS_1-Year_PUMS.html#accordion-container-accordion-4ad4d2b1b3).\"\n",
    "- [Link to the API documentation](https://www.census.gov/data/developers/data-sets/census-microdata-api.ACS_1-Year_PUMS.html#accordion-container-accordion-4ad4d2b1b3)\n",
    "\n",
    "#### Constructing Your Query\n",
    "\n",
    "Before you jump into writing the API call in Python, use the available documentation to make sense of how the URL is structured, what variables are included in the dataset, etc. \n",
    "\n",
    "Note that for the Census Bureau API, your key is part of the URL. Concatenation and a string variable can be helpful here.\n",
    "\n",
    "Let's break down one of the example URLs included in the documentation for this survey.\n",
    "\n",
    "```\n",
    "https://api.census.gov/data/2022/acs/acs1/pums?get=SEX,PWGTP,MAR&SCHL=24&key=YOUR_KEY_GOES_HERE\n",
    "```\n",
    "\n",
    "We can start to break down the pieces of information included here:\n",
    "- Root: `https://api.census.gov/data`\n",
    "- Survey: `acs` (American Community Survey)\n",
    "- Coverage: `acs1` (1 year estimates)\n",
    "- Subset: `pums` (Public Use Microdata Sample)\n",
    "- Query: `?get=` (prefix for selecting specific variables)\n",
    "- Variables: `SEX,PWGTP,MAR&SCHL=24` (codes for specific variables, from [the documentation](https://api.census.gov/data/2022/acs/acs1/pums/variables.html))\n",
    "  * NOTE: In this example, the URL is structured to select all values for the `SEX`, `PWGTP`, `MAR` variables. This query will only return records where the `SCHL` variable value is `24`.\n",
    "  * In a scenario where we are not filtering for specific variable values, we could just include variable names separated by commas (i.e. `SEX, PWGTP, MAR, SCHL`)\n",
    "- `&key=` (prefix for entering your API key)\n",
    "\n",
    "#### Writing the API Call\n",
    "\n",
    "Remember our general API workflow:\n",
    "- Customize query URL\n",
    "- Load response in Python\n",
    "- Parse the data structure to isolate the data we want\n",
    "- Store that data as a Pandas `DataFrame`\n",
    "\n",
    "```Python\n",
    "import requests, pandas as pd # import statements\n",
    "key = \"\" # add your key to make this a string variable\n",
    "url = f\"https://api.census.gov/data/2022/acs/acs1/pums?get=SEX,PWGTP,MAR&SCHL=24&key={key}\" # use f strings and concatenation to construct the query\n",
    "r = requests.get(url) # requests data\n",
    "data = r.json() # store response\n",
    "df = pd.DataFrame(data[1:], columns=data[0]) # create the dataframe, making the first sublist the column headers, and starting with the first row of data to avoid duplicating headers)\n",
    "print(df) # show output\n",
    "```\n",
    "\n",
    "#### Other Resources\n",
    "\n",
    "This is an example query- there are lots of ways to customize Census Bureau API requests. Check out [`census-docs`](https://census-docs.com/) and the Census Bureau's API documentation for more info.\n",
    "- For example, if we just wanted data for Indiana, we could add the `in` parameter to your URL: 'api.census.gov/data/2022/acs/acs1/pums?get=SEX,PWGTP,MAR&SCHL=24&in=state:17&key=`\n",
    "\n",
    "Some datasets include `Groups`, which let you return multiple related variables.\n",
    "- [Link to more information on groups](https://www.census.gov/data/developers/updates/groups-functionality.html)\n",
    "\n",
    "DataMade also has a Python wrapper for the Census Bureau APIs, which is designed to streamline programming workflows.\n",
    "- [Link to more information](https://github.com/datamade/census)\n",
    "\n",
    "The Census Bureau APIs can be overwhelming, but they're a powerful tool for accessing data. Wading through the documentation is worth your time.\n",
    "\n",
    "#### Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02315192",
   "metadata": {},
   "source": [
    "Q4: \n",
    "\n",
    "Write your own Census Bureau API call. You could use the same dataset and modify the year, variables, geography scope, etc.\n",
    "\n",
    "You could also explore other datasets.\n",
    "\n",
    "Starting to make sense of the documentation\n",
    "\n",
    "Getting the data in Python\n",
    "\n",
    "Storing as pandas df\n",
    "\n",
    "## Working With Unstructured Text\n",
    "\n",
    "START HERE\n",
    "- Elisevier video: https://youtu.be/I3cjbB38Z4A?si=N3hv-j7dkOHDkXED\n",
    "- Elsivier video: https://youtu.be/xxqrIZyKKuk?si=ncOxIy5EZufVpmkf\n",
    "\n",
    "EXPLANATION FOR UNSTRUCTURED TEXT\n",
    "\n",
    "If you took Elements of Computing I with Prof. Walden, you had some exposure to how we can start to work with unstructured text, looking at things like term relationships, frequency, etc. Web scraping is a great workflow for information that is online in web pages. But what about text information contained in documents? \n",
    "\n",
    "Enter Optical Character Recognition, or OCR.\n",
    "\n",
    "\"Optical character recognition (OCR) is sometimes referred to as text recognition. An OCR program extracts and repurposes data from scanned documents, camera images and image-only pdfs. OCR software singles out letters on the image, puts them into words and then puts the words into sentences, thus enabling access to and editing of the original content...OCR systems use a combination of hardware and software to convert physical, printed documents into machine-readable text\" (IBM Cloud Education, \"[What is Optical Character Recognition?](https://www.ibm.com/blog/optical-character-recognition/)\", 5 January 2022)\n",
    "\n",
    "OCR is an example of computer vision in action. A full exploration of OCR or computer vision is outside the scope of this course. We're going to focus on some Python workflows for extracting text and data from PDF documents that already contain text.\n",
    "- [Link to a (messy) Jupyter Notebook that walks through some OCR foundations](https://drive.google.com/file/d/1WbGTAvs1WCGrC5XZeADyhminFn8QMEHT/view?usp=sharing)\n",
    "\n",
    "We're going to be using a couple Python libraries built for extracting text and tables from PDF files.\n",
    "- [`pypdf`](https://pypdf.readthedocs.io/en/stable/index.html)\n",
    "- [`tabula-py`](https://tabula-py.readthedocs.io/en/latest/)\n",
    "\n",
    "And we'll be testing out these workflows on a PDF file from the [City of South Bend's Document Search Center](http://docs.southbendin.gov/WebLink/Welcome.aspx?cr=1). I'm looking at the [Inclusive Procurement & Contracting Board's 2020 report](http://docs.southbendin.gov/WebLink/0/edoc/335114/2020%20Annual%20Inclusion%20Contracting%20Procurement%20Report%203-27-21_.pdf).\n",
    "\n",
    "### Python Workflows\n",
    "\n",
    "We'll start by installing and importing these packages.\n",
    "\n",
    "```Python\n",
    "!pip install pypdf tabula-py\n",
    "```\n",
    "\n",
    "```Python\n",
    "# import statements\n",
    "from pypdf importPdfReader\n",
    "import tabula\n",
    "```\n",
    "\n",
    "Next, we'll use `PdfReader` to extract text.\n",
    "\n",
    "```Python\n",
    "reader = PdfReader(\"pdf20.pdf\") # load renamed file as reader object\n",
    "page = reader.pages[2] # isolate single page\n",
    "print(page.extract_text()) # show extracted text output\n",
    "```\n",
    "\n",
    "To iterate over all pages in the document and write the contents to a `.txt` file:\n",
    "\n",
    "```Python\n",
    "with open(\"output.txt\", \"a\") as f: # create file\n",
    "  for p in reader.pages: # iterate over pages\n",
    "    f.write(p.extract_text()) # extract text and write to file\n",
    "```\n",
    "\n",
    "Folks may be wondering how we might handle tables or tabular data in a PDF file. We can use Tabula's `.read_pdf()` function to extract all tables in our document as Pandas `DataFrames`.\n",
    "\n",
    "```Python\n",
    "dfs = tabula.read_pdf(\"pdf20.pdf\", pages=\"all\") # load file, isolate dataframes\n",
    "dfs[5].to_csv(\"output.csv\", index=False) # write single dataframe to CSV file\n",
    "dfs[5] # show single dataframe\n",
    "```\n",
    "\n",
    "There's more we probably want to do with output, but this gets us started.\n",
    "\n",
    "### Application\n",
    "\n",
    "Q5:\n",
    "\n",
    "Find another SB file. \n",
    "\n",
    "Skim the document, what types of information are there.\n",
    "\n",
    "Extract text to txt file\n",
    "\n",
    "Extract tables, write one to CSV file\n",
    "\n",
    "Where you'd want to go next, other data processing steps\n",
    "\n",
    "## How to Submit This Lab & Show Your Work\n",
    "\n",
    "## Lab Notebook Questions\n",
    "`"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.16.1"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   14,
   274,
   531
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}